---
title: ResNet50 을 활용한 돼지 도체의 성별 예측
author: Hyo-Jun Lee
date: 2021-03-02 17:00:00 +0900
categories: [Computer-Vision]
tags: [Computer-Vision, Animal-Science]
math: true
---

<small>사용된 코드는 아래의 url 에서 확인 하실 수 있습니다. The codes used in the poster can be found at **https://github.com/gywns6287/pig_sex_classifier**
</small>

이번 프로젝트에서는 컴퓨터 비전 영역의 성공한 딥러닝 모델인 ResNet50 를 활용하여 돼지 도체의 성별을 예측하는 모델을 구현해 보고자 합니다.

## ResNet50

### Skip connection
ResNet 은 ImageNet 영상 인식 대회에 2015년에 승리를 한 네트워크로 현재 많은 CNN 모델의 트렌드를 만든 모델입니다. ResNet의 핵심 요소는 바로 스킵 연결 (skip connection) 입니다. 숏컷 연결 (shortcut connection) 이라고도 불리는 이 기법은 말 그대로 특정 층에 존재하는 출력물을 다음 합성곱 층을 통과한 출력물에 더해주는 연결을 뜻 합니다. 

![image](https://user-images.githubusercontent.com/71325306/109637429-215a3880-7b90-11eb-9014-3a8fdbbe299b.png)

일반적인 신경망을 훈련시킬 때는 목적 함수 $$h(x)$$ 를 모델링 하는 것을 목표로 하게 됩니다. 하지만, 네트워크의 출력이 입력 부분과 더해지게 된다면 네트워크는 $$h(x)$$ 를 학습 하는 대신 $$f(x)=h(x)-x$$ 를 학습하게 될 것입니다. 다시 생각해보면 이 $$f(x)$$는 바로 모델 $$h(x)$$ 의 잔차(residual) 라고도 볼 수 있습니다. 이것이 바로 이 네트워크가 Res(residual) Net 이라고 불리는 이유라고 할 수 있습니다.

### Model architecture

![image](https://user-images.githubusercontent.com/71325306/109642130-ef4bd500-7b95-11eb-8e75-28fd26f73a0a.png)

위의 그림은 ResNet의 구조에 대한 간략한 모식도 입니다. ResNet의 잔차블록은 채널의 개수를 줄여주는 첫번째 층 (합성곱 1x1) 이후 합성 곱 연산을 담당해주는 두번째 층 (합성곱 3x3) 마지막으로 채널의 개수를 다시 늘려주는 층 (합성곱 1x1) 총 세개의 층으로 병목 (bottle neck) 구조로 구현되어 있습니다. ResNet 은 이러한 잔차 블록을 얼마나 쌓는가에 따라 ResNet36, ResNet50, ResNet101 층 등 다양한 형태를 취할 수 있습니다.

## Class activation map

딥 러닝의 가장 큰 단점 중 하나는 바로 모델이 영상으로부터 의사 결정을 하게 된 원인을 파악하기 힘들다는 점입니다. Bolei et al 은 이러한 문제점을 해결하기 위해 Class activation map(CAM) 을 제시하였습니다.

![image](https://user-images.githubusercontent.com/71325306/109638768-c295be80-7b91-11eb-8d3a-02b701e937d7.png)

위의 그림은 CAM의 원리를 보여주고 있습니다. 딥 러닝 모델의 마지막 출력 부문을 GAP(Global Average Pooling)으로 변환 시켜 주어 마지막 합성 곱 층에서 출력되는 feature channel 들의 weight를  학습과정에서 모델이 계산할 수 있게 해줍니다. 후에 계산된 weights와 마지막 합성 곱 층의 출력 feature channel 들을 곱한 후 모두 더해주면 모델이 의사결정을 하게 된 요인을 파악하기 위한 Class activation map을 얻을 수 있게 됩니다.

## Pig sex estimation

돼지의 성별은 돼지고기의 가격을 결정하게 되는 주요한 요인입니다. 현재 돼지의 성별은 돼지의 도축 과정 중 숙련된 판정사에 의해 판정되고 있습니다. 하지만 사람이 일일이 모든 돼지고기를 판정 하는 것은 생산과정에서 많은 시간과 인력을 요구하게 됩니다. 이에 돼지고기 생성 공정에 인공지능을 활용한 자동 돼지 성별 판독기를 도입한다면, 더 효율적인 돼지고기 생산이 가능해질 것입니다.

### Implementation

우선 ResNet50 모델을 구현해 봅시다. 이번 프로젝트에서는 keras applications 에서 제공해 주는 ResNet50 을 메서드를 활용해 간편히 불러왔습니다. 함수에서 `include_top = False` 를 선언해 주어 ResNet50 의 출력 부분을 제외한 부분을 호출합니다. 이후 만들어진 `base` 모델에 `inputs` 을 통과시켜주어 그래프를 연결해 줍니다. 모델의 마지막 층은 CAM을 생성하기 위해 클래스의 개수 만큼 Channel을 형성하게 디자인해 주며 `GlobalAveragePooling2D()` 역시도 추가해 줍니다. 
```python
import tensorflow as tf
from tensorflow.keras.layers import *
from tensorflow.keras import Model
from tensorflow.keras.applications import ResNet50
from data_loader import *

def ResNet50_CAM(input_shape = (224,224,3)):
    
    base = ResNet50(include_top = False, input_shape = input_shape)
    
    inputs = Input(input_shape)
    
    x = base(inputs)
    x = Conv2D(class_num, (3,3), activation = 'relu')(x)
    
    x = GlobalAveragePooling2D()(x)
    out =  Dense(class_num, activation = 'softmax')(x)
    
    return Model(inputs,out)
```

---
모델의 학습에 사용할 하이퍼파라미터들을 세팅해 줍니다. 여기서 `crop_point` 를 지정하여 다음에 나올 코드에서 돼지 이미지에서 성별 분류에 필요없는 전방 0.4 영역을 잘라내 주게 됩니다.
```python
train_path = 'data\\train'
test_path = 'data\\test'
out = 'results'

epochs = 100
batch_size = 16
input_shape = (224,224,3)
crop_point = (0.4,0.4)
lr = 1e-05
class_num = 3
```

---
`crop_loader` 는 이미지를 잘라 낸 후 생성해주는 제네레이터로 **https://github.com/gywns6287/pig_sex_classifier** 의 코드에서 자세히 확인할 수 있습니다. 코드 실행 결과 이번 프로젝트에서 활용할 데이터는 train: 5545장, test: 1384장으로 이루어져 있는 것을 확인 할 수 있습니다.
```python
#Load Data
train = crop_loader(train_path, crop_point, target_size = input_shape[:-1], batch_size = batch_size , mode = 'train')
test = crop_loader(test_path, crop_point, target_size = input_shape[:-1], batch_size = batch_size , mode = 'test')
```
    Found 5545 images belonging to 3 classes.
    Found 1384 images belonging to 3 classes.

---
모델을 학습 시켜 줍니다. 이번 프로젝트에서는 모델을 총 100회 학습하였습니다.
```python
#Load Model
model = ResNet50_CAM(input_shape = input_shape)
model.compile(tf.keras.optimizers.Adam(learning_rate = lr), "categorical_crossentropy", metrics = ['accuracy'])
ckp = tf.keras.callbacks.ModelCheckpoint(out+'\\'+"weights.h5", 
                                monitor='loss',
                                verbose=1, 
                                save_best_only=True)

#Train Model
model.fit(train.gen,
        steps_per_epoch = len(train.geninfo),
        epochs = epochs, 
        callbacks= [ckp],
        validation_data = test.gen,
        validation_steps = len(test.geninfo))
```
    Epoch 1/100
    347/347 [==============================] - ETA: 0s - loss: 0.8445 - accuracy: 0.4885
    Epoch 00001: loss improved from inf to 0.84451, saving model to results\weights.h5
    347/347 [==============================] - 158s 454ms/step - loss: 0.8445 - accuracy: 0.4885 - val_loss: 1.0767 - val_accuracy: 0.4227
    Epoch 2/100
    347/347 [==============================] - ETA: 0s - loss: 0.6643 - accuracy: 0.6646
    Epoch 00002: loss improved from 0.84451 to 0.66428, saving model to results\weights.h5
    347/347 [==============================] - 158s 455ms/step - loss: 0.6643 - accuracy: 0.6646 - val_loss: 1.0897 - val_accuracy: 0.4386
    
    ...

    Epoch 100/100
    347/347 [==============================] - ETA: 0s - loss: 0.3090 - accuracy: 0.9980
    Epoch 00100: loss improved from 0.31035 to 0.30902, saving model to results\weights.h5
    347/347 [==============================] - 154s 445ms/step - loss: 0.3090 - accuracy: 0.9980 - val_loss: 0.3237 - val_accuracy: 0.9957

    <tensorflow.python.keras.callbacks.History at 0x245e2d787f0>

---
모델의 성능을 평가해 보겠습니다. 다시 한번 `test` 제네레이터를 생성한 후 각 원-핫 코딩의 인덱스가 어떤 클래스를 지칭하는지를 `class_dic` 에 저장합니다.
```python
#evaludation performance
test = crop_loader(test_path, crop_point, target_size = input_shape[:-1], batch_size = batch_size , mode = 'test')
class_dic = {v:k for k,v in test.geninfo.class_indices.items()}      
class_dic
```
    Found 1384 images belonging to 3 classes.
    {0: 'boar', 1: 'female', 2: 'male'}

---
test set의 정답 성별을 `true_class` 에 저장합니다. 이후 모델의 `predict` 메서드를 활용하여 예측 성별을 계산해 `pred_class` 에 저장합니다.
```python
true_class = np.array([class_dic[i] for i in test.geninfo.classes])

pred = model.predict(test.gen, steps = len(test.geninfo), verbose = 1)
pred_class = np.array([class_dic[i] for i in np.argmax(pred, axis = 1)])

true_class,pred_class
```
    87/87 [==============================] - 12s 137ms/step 
    (array(['boar', 'boar', 'boar', ..., 'male', 'male', 'male'], dtype='<U6'),
     array(['boar', 'boar', 'boar', ..., 'boar', 'boar', 'boar'], dtype='<U6'))

---
`sklaen.metrics` 에서 제공하는 메서드를 활용해 모델의 성능을 평가해 보았습니다. 정확도는 약 96.1%, F1 score 는 95.1%로 제법 좋은 수준의 모델을 만드는 데 성공 했습니다.
```python
import sklearn.metrics

print('accuracy:',sklearn.metrics.accuracy_score(pred_class,true_class))
print('F1 score:',sklearn.metrics.f1_score(pred_class,true_class,average = 'macro'))
```
    accuracy: 0.9609826589595376
    F1 score: 0.950727719040373

---
다음으로 임의의 사진을 불러와 예측하고 CAM을 생성해 보겠습니다. `female` 성별 중 임의의 사진 1장을 불러와 `image_crop` 함수를 사용해 이미지를 잘라 주었습니다. 이 함수 역시 깃허브 코드에서 확인할 수 있습니다. 잘린 이미지를 확인합니다.
```python
sex = 'female'

import random
random_img = random.sample(os.listdir('data\\test\\'+sex), k = 1)[0]
img_path = 'data\\test\\' + sex + '\\' + random_img

img = image_crop(img_path, crop_point, target_size = (224,224))
img
```
![output_8_0](https://user-images.githubusercontent.com/71325306/109754570-9c226280-7c27-11eb-93cc-b672c799c1cc.png)

---
이미지를 `np.array` 로 변형시켜 준 뒤 `model.predict` 메서드를 통해 결과를 예측합니다. `pred` 는 이미지의 예측 확률 벡터를 가지고 있을 것 입니다. 가장 높은 확률값의 위치를 `idx` 에 할당합니다.
```python
#estimate pig sex
img_arr = np.array(img.resize((224,224)))/255.
pred = model.predict(img_arr.reshape((1,)+input_shape))
idx = np.argmax(pred)
```

---
CAM 의 생성을 위해 model 의 마지막 feature_channel 을 생성하는 층을 분할 해 새로운 모델 `CAM_model` 을 만듭니다. 이후 만들어진 모델에 `img_arr` 를 인풋 해 `feature_channel` 을 만들어 줍니다. 만들어진 채널별로 weights를 곱해주기 위해 모델 마지막 층의 weights를 `weights` 에 선언해 줍니다. 최종적으로 각 feature_channel 과 weights 의 곱을 더해주어 `CAM` 을 만들어 냅니다. 
```python
#separate model for generating CAM
CAM_model = Model(inputs = model.inputs, outputs = model.layers[-3].output)

#generate CAM
feature_channel = CAM_model(img_arr.reshape((1,)+input_shape))
weights = model.layers[-1].weights[0].numpy()

CAM = np.zeros((feature_channel.shape[1:3]))
for w in range(len(weights[idx])):
    #sum weights * feature channel    
    CAM += feature_channel[0][:,:,w] * weights[:,idx][w]
```

---
만들어진 `CAM` 을 최솟값은 0 최댓값은 1 이 되도록 정규화 시켜 줍니다.
```python
#Normalization CAM
CAM = CAM - np.min(CAM)
CAM = CAM/np.max(CAM)
CAM = Image.fromarray(np.uint8(255 * CAM)).resize((224,224))
```

---
`matplotlib.pyplot` 을 import 해주어 img 와 CAM 을 그린 후 결과를 확인합니다. ResNet50 가 실제로 돼지의 성별에 주요한 도체의 후면부에 집중에서 성별을 결정짓는 사실을 확인할 수 있습니다. 성별 예측 역시 100% 확률 값으로 female로 잘 예측 해 내는 것을 확인할 수 있습니다.
```python
import matplotlib.pyplot as plt

plt.imshow(img)
plt.imshow(CAM, cmap='jet',alpha = 0.6)
plt.axis('off')
plt.show()

print(img_path)
print('-'*60)
print('True:',sex)
print('Pred:',class_dic[idx],'({0}%)'.format(np.max(pred)*100))
```
![output_12_0](https://user-images.githubusercontent.com/71325306/109755335-2b7c4580-7c29-11eb-883f-3d2bc29e1fb8.png)

    data\test\female\B-2020.07.30-09-34-20-0000421 C1.tif
    ------------------------------------------------------------
    True: female
    Pred: female (100.0%)

## Reference
He, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 770-778).

Zhou, B., Khosla, A., Lapedriza, A., Oliva, A., & Torralba, A. (2016). Learning deep features for discriminative localization. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 2921-2929).